{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG using `llama-index`\n",
    "\n",
    "##### Install dependencies (`llama-index, chromaDB, llama-index-embeddings & ollama dependencies`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index chromadb llama-index-llms-ollama langchain-community  llama-index-embeddings-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Load the External Knowedge Base into a document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"KB\").load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1a Create a **ChromaDB** *(Vector DB)* instance and initialize a `storage_context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "chroma_client = chromadb.PersistentClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"concepts-in-tamil\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1b Set Ollama as the LLM to be used both for embedding & for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "Settings.llm = Ollama(model='dolphin-mistral:latest', request_timeout=60.0)\n",
    "lc_ollama_embeddings = OllamaEmbeddings(model=\"dolphin-mistral:latest\")\n",
    "Settings.embed_model = LangchainEmbedding(lc_ollama_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 & Step 3 Generate Index based on the knowledge base and store the index in vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took around 4m 27s to build this index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 & 5 Now we are ready to query this index along with the input search string from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " From the provided context information, we cannot precisely determine Microsoft's total cloud revenue for fiscal year 2023. However, we can find some information related to specific aspects of their business that involve clouds services. For instance, we know they earned \"Revenue from Server products and cloud services\" which includes Azure and other cloud services as well as their SQL Server, Windows Server, Visual Studio, System Center, and GitHub.\n",
      "\n",
      "In the 2023 fiscal year, the revenue generated from this segment is reported to be $146,052 million. Although it does not explicitly mention \"cloud revenue,\" a significant portion of this revenue comes from cloud-related offerings. In order to pinpoint exactly how much of this total revenue can be attributed specifically to their Azure and other cloud services, you would need more detailed financial data which is beyond the scope of provided context information.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What was Microsoft's cloud revenue in fiscal year 2023?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Customization if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now to get improved results, we can customize the index retriver to get more context and also ask the LLM to summarize the answer more throughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the provided information, Microsoft's cloud revenue in fiscal year 2023 was $111.6 billion. This amount is primarily included in Server products and cloud services, Office products and cloud services, LinkedIn, and Dynamics in the table above. To be more specific, this revenue figure is found on page 77 of the annual report under the section titled \"Revenue from LinkedIn, including Talent Solutions, Marketing Solutions, Premium Subscriptions, and Sales Solutions,\" where it states that Microsoft's \"Microsoft Cloud revenue... for fiscal year 2023 was $111.6 billion.\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=5, response_mode=\"tree_summarize\")\n",
    "response = query_engine.query(\"What was Microsoft's cloud revenue in fiscal year 2023?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more customization options, look [here](https://docs.llamaindex.ai/en/stable/getting_started/customization.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
